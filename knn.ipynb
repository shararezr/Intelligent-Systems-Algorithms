{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5452\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv('train_data.csv')\n",
    "data_test = pd.read_csv('test_data.csv')\n",
    "test_label = pd.read_csv('test_labels.csv')\n",
    "train_label = pd.read_csv('train_labels.csv')\n",
    "print(data_train.idxmax()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(data_test)\n",
    "X_test_std = (X_test - np.mean(X_test,axis=0)) / np.std(X_test,axis=0)\n",
    "Y_test = np.array(test_label)\n",
    "X_train = np.array(data_train)\n",
    "X_train_std = (X_train - np.mean(X_train,axis=0)) / np.std(X_train,axis=0)\n",
    "Y_train = np.array(train_label)\n",
    "k_neighbours = [2,5,10,50]\n",
    "k_accuracy = []\n",
    "\n",
    "\n",
    "def knn(train_data, train_label, test_sample, k_neighbours, typeofmetric):\n",
    "    \n",
    "    distance = pd.DataFrame(data=np.sqrt(((train_data -test_sample)**2 ).sum(axis=1)),columns=['data'])\n",
    "    \n",
    "    if(typeofmetric == \"norm1\"):\n",
    "        distance = pd.DataFrame(data=((train_data -test_sample)).max(axis=1),columns=['data'])\n",
    "    if(typeofmetric== \"norm2\"):\n",
    "        distance = pd.DataFrame(data=(abs(train_data -test_sample) ).sum(axis=1),columns=['data'])\n",
    "       \n",
    "    distance['label'] = train_label\n",
    "    distance = distance.sort_values(by='data')\n",
    "    neighbors = distance.iloc[0:k_neighbours,:]\n",
    "    label = neighbors['label'].value_counts().idxmax()\n",
    "    return label\n",
    "\n",
    "\n",
    "\n",
    "def main(X_train, Y_train, X_test, Y_test, typeofmeric):\n",
    "    for k in k_neighbours:\n",
    "        cnt = 0\n",
    "        for idsample in range(X_test.shape[0]):\n",
    "            label=knn(X_train, Y_train, X_test[idsample], k, typeofmeric)\n",
    "            if(label == Y_test[idsample]):\n",
    "                cnt = cnt+1\n",
    "\n",
    "        k_accuracy.append(cnt/Y_test.shape[0]*100)\n",
    "        print(f'accuracy for k={k} is {k_accuracy[-1]}!') \n",
    "\n",
    "def mainforbestk(X_train, Y_train, X_test, Y_test, typeofmeric, k):\n",
    "    cnt = 0\n",
    "    k_accuracy = 0\n",
    "    for idsample in range(X_test.shape[0]):\n",
    "        label=knn(X_train, Y_train, X_test[idsample], k, typeofmeric)\n",
    "        if(label == Y_test[idsample]):\n",
    "            cnt = cnt+1\n",
    "\n",
    "    k_accuracy = cnt/Y_test.shape[0]*100\n",
    "    print(f'accuracy for k={k} is {k_accuracy}!') \n",
    "\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for k=2 is 12.832237782223494!\n",
      "accuracy for k=5 is 13.889682766504716!\n",
      "accuracy for k=10 is 13.43240925978851!\n",
      "accuracy for k=50 is 14.004001143183768!\n"
     ]
    }
   ],
   "source": [
    "main(X_train, Y_train, X_test, Y_test, 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For classification algorithms like KNN, we measure the distances between pairs of samples and these distances are influenced by the measurement units also. For example: Letâ€™s say, we are applying KNN on a data set having 3 features.First feature ranging from 1-10, second from 1-20 and the last one ranging from 1-1000. In this case, most of the clusters will be generated based on the last feature as the difference between 1 to 10 and 1-20 are smaller as compared to 1-1000. To avoid this miss classification, we should normalize the feature variables.If the scale of features is very different then normalization is required. This is because the distance calculation done in KNN uses feature values. When the one feature values are large than other, that feature will dominate the distance hence the outcome of the KNN!\n",
    "So as you can see here the accuracy of the classification is very low,Now we do the classification again for standardized data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for k=2 is 71.33466704772792!\n",
      "accuracy for k=5 is 74.70705915975994!\n",
      "accuracy for k=10 is 73.47813661046013!\n",
      "accuracy for k=50 is 61.589025435838806!\n"
     ]
    }
   ],
   "source": [
    "main(X_train_std, Y_train, X_test_std, Y_test, 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now you see the difference, here the best accuracy is for k=5!                                                                                                                        \n",
    "For very low values and very high values of k, the accuracy comes down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for standardized data we have: \n",
      "accuracy for k=5 is 49.299799942840814!\n",
      "before standardized data we have: \n",
      "accuracy for k=5 is 42.955130037153474!\n"
     ]
    }
   ],
   "source": [
    "print(\"for standardized data we have: \")\n",
    "mainforbestk(X_train_std, Y_train, X_test_std, Y_test, 'norm1', 5)\n",
    "print(\"before standardized data we have: \")\n",
    "mainforbestk(X_train, Y_train, X_test, Y_test, 'norm1', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for standardized data we have: \n",
      "accuracy for k=5 is 75.99314089739926!\n",
      "before standardized data we have: \n",
      "accuracy for k=5 is 38.496713346670475!\n"
     ]
    }
   ],
   "source": [
    "print(\"for standardized data we have: \")\n",
    "mainforbestk(X_train_std, Y_train, X_test_std, Y_test, 'norm2', 5)\n",
    "print(\"before standardized data we have: \")\n",
    "mainforbestk(X_train, Y_train, X_test, Y_test, 'norm2', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sharare zolghadr(610395109)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
